###############################################################################
## PHYSIOTRACK PROJECT PARAMETERS                                            ##
###############################################################################

# Configure your project parameters here

# Then open a command prompt and enter:
# physiotrack --config Config_demo.toml


[project]
video_input = 'demo.mp4'   # 'webcam' or '<video_path.ext>', or ['video1_path.mp4', 'video2_path.avi>', ...]
                           # On Windows, replace '\' with '/'
                           # Beware that images won't be saved if paths contain non ASCII characters.
px_to_m_from_person_id = 2    # Person to use for pixels to meters conversion (not used if a calibration file is provided)
px_to_m_person_height = 1.65  # Height of the reference person in meters (for pixels -> meters conversion). 
visible_side = ['front', 'none', 'auto']  # Choose visible side among ['right', 'left', 'front', 'back', 'auto', 'none']. String or list of strings.
                  # if 'auto', will be either 'left', 'right', or 'front' depending on the direction of the motion
                  # if 'none', coordinates will be left in 2D rather than 3D
load_trc_px = ''  # If you do not want to recalculate pose, load it from a trc file (in px, not in m)
compare = false   # Not implemented yet

# Video parameters
time_range = []   # [] for the whole video, or [start_time, end_time] (in seconds), or [[start_time1, end_time1], [start_time2, end_time2], ...]
                  # Time ranges can be different for each video. 
video_dir = ''    # If empty, video dir is current dir

# Webcam parameters
webcam_id = 0 # your webcam id (0 is default)
input_size = [1280, 720] # [W, H]. Lower resolution will be faster but less precise.


[process]
multiperson = true   # Saving the motions of all the persons detected and tracked in the video. 
                     # If false, the person saved will be the one with the highest sum of keypoint scores over the video
show_realtime_results = true
save_vid = true
save_img = true
save_pose = true
calculate_angles = true
save_angles = true
result_dir = '' # If empty, project dir is current dir


##########################
# ADVANCED CONFIGURATION #
##########################
[pose]
# Slow motion factor
slowmo_factor = 1       # 1 for normal speed. For a video recorded at 240 fps and exported to 30 fps, it would be 240/30 = 8

# Pose detection parameters
pose_model = 'Body_with_feet'  #With RTMLib: 
                         # - Body_with_feet (default HALPE_26 model), 
                         # - Whole_body_wrist (COCO_133_WRIST: body + feet + 2 hand_points), 
                         # - Whole_body (COCO_133: body + feet + hands), 
                         # - Body (COCO_17). Potentially better for performance and real-time but provides less detailed information.

mode = 'performance' # 'lightweight', 'balanced', 'performance', or """{dictionary}""" (see below)

# A dictionary (WITHIN THREE DOUBLE QUOTES) allows you to manually select the person detection (if top_down approach) and/or pose estimation models. 
# Models can be local paths or URLs.
# Make sure the input_sizes are within square brackets, and that they are in the opposite order from the one in the model path (for example, it would be [192,256] for rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.zip). 
# If your pose_model is not provided in skeletons.py, you may have to create your own one (see example at the end of the file).
# Example, equivalent to mode='balanced':
# mode = """{'det_class':'YOLOX',
#          'det_model':'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/yolox_m_8xb8-300e_humanart-c2c7a14a.zip',
#          'det_input_size':[640, 640],
#          'pose_class':'RTMPose',
#          'pose_model':'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/rtmpose-m_simcc-body7_pt-body7-halpe26_700e-256x192-4d3e73dd_20230605.zip',
#          'pose_input_size':[192,256]}"""

det_frequency = 4       # Run person detection only every N frames, and inbetween track previously detected bounding boxes (keypoint detection is still run on all frames). 
                        # Equal to or greater than 1, can be as high as you want in simple uncrowded cases. Much faster, but might be less accurate. 
device = 'auto' # 'auto', 'CPU', 'CUDA', 'MPS', 'ROCM'
backend = 'auto' # 'auto', 'openvino', 'onnxruntime', 'opencv'
tracking_mode = 'physiotrack' # 'physiotrack' or 'deepsort'. 'deepsort' is slower, harder to parametrize but can be more robust if correctly tuned

# Processing parameters
keypoint_likelihood_threshold = 0.3 # Keypoints whose likelihood is lower will not be taken into account
average_likelihood_threshold = 0.5  # Person will be ignored if average likelihood of good keypoints is lower than this value
keypoint_number_threshold = 0.3     # Person will be ignored if the number of good keypoints (above keypoint_likelihood_threshold) is less than this fraction


[px_to_meters_conversion]
# Pixel to meters conversion
to_meters = true
make_c3d = true
save_calib = true

# If conversion from px_to_m_person_height
floor_angle = 'auto'    # 'auto' or a value in degrees, eg 2.3. If 'auto', estimated from the line formed by the toes when they are on the ground (where speed = 0)
xy_origin = ['auto']    # ['auto'] or [px_x,px_y]. N.B.: px_y points downwards. If ['auto'], direction estimated from the start to the end of the line formed by the toes when they are on the ground

# If conversion from a calibration file
calib_file = ''         # Calibration in the appropriate format. 'calib_demo.toml', or '' if not available


[angles]
display_angle_values_on = ['body', 'list'] # 'body', 'list', ['body', 'list'], 'none'. Display angle values on the body, as a list in the upper left of the image, both, or do not display them.
fontSize = 0.3

# Select joint angles among
# ['Right ankle', 'Left ankle', 'Right knee', 'Left knee', 'Right hip', 'Left hip', 'Right shoulder', 'Left shoulder', 'Right elbow', 'Left elbow', 'Right wrist', 'Left wrist']
joint_angles = ['Right ankle', 'Left ankle', 'Right knee', 'Left knee', 'Right hip', 'Left hip', 'Right shoulder', 'Left shoulder', 'Right elbow', 'Left elbow', 'Right wrist', 'Left wrist']
# Select segment angles among
# ['Right foot', 'Left foot', 'Right shank', 'Left shank', 'Right thigh', 'Left thigh', 'Pelvis', 'Trunk', 'Shoulders', 'Head', 'Right arm', 'Left arm', 'Right forearm', 'Left forearm']
segment_angles = ['Right foot', 'Left foot', 'Right shank', 'Left shank', 'Right thigh', 'Left thigh', 'Pelvis', 'Trunk', 'Shoulders', 'Head', 'Right arm', 'Left arm', 'Right forearm', 'Left forearm']

# Processing parameters
flip_left_right = true  # Same angles whether the participant faces left/right. Set it to false if you want timeseries to be continuous even when the participent switches their stance.
correct_segment_angles_with_floor_angle = true # If the camera is tilted, corrects segment angles as regards to the floor angle. Set to false is the floor is tilted instead


[post-processing]
interpolate = true
interp_gap_smaller_than = 10        # do not interpolate bigger gaps
fill_large_gaps_with = 'last_value' # 'last_value', 'nan', or 'zeros' 

filter = true
show_graphs = true            # Show plots of raw and processed results
filter_type = 'butterworth'   # butterworth, gaussian, LOESS, median
   [post-processing.butterworth]
   order = 4 
   cut_off_frequency = 6 # Hz # Will be divided by slowmo_factor to be equivalent to non slowed-down video
   [post-processing.gaussian]
   sigma_kernel = 1 #px
   [post-processing.loess]
   nb_values_used = 5 # = fraction of data used * nb frames
   [post-processing.median]
   kernel_size = 3


[kinematics]
do_ik = false # Not implemented yet
use_augmentation = false  # Not implemented yet
use_contacts_muscles = true # Not implemented yet
participant_mass = [67.0, 55.0] # kg # defaults to 70 if not provided. No influence on kinematics (motion), only on kinetics (forces)
right_left_symmetry = true # true or false (lowercase) # Set to false only if you have good reasons to think the participant is not symmetrical (e.g. prosthetic limb)

# Choosing best frames to scale the model
default_height = 1.7 # meters # If automatic height calculation did not work, this value is used
fastest_frames_to_remove_percent = 0.1 # Frames with high speed are considered as outliers
close_to_zero_speed_px = 50 # Sum for all keypoints: about 50 px/frame
close_to_zero_speed_m = 0.2 # Sum for all keypoints: 0.2 m/frame
large_hip_knee_angles = 45 # Hip and knee angles below this value are considered as imprecise
trimmed_extrema_percent = 0.5 # Proportion of the most extreme segment values to remove before calculating their mean)
osim_setup_path = '../OpenSim_setup'


[logging]
use_custom_logging = false # if integrated in an API that already has logging